{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is not complicate, so for the assumption, I respect the accurancy of the data I get from morse and the weather report website. What I should do with ETL is mostly to transform for further use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The the beginning, I did ETL with general coding, such web crawling, backfill and transform.\n",
    "When it comes to analysis, I used Spark Pipeline for one hot encoding and assembling feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Morse Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "morse=pd.read_csv('morse.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>local_created_at</th>\n",
       "      <th>item_name</th>\n",
       "      <th>net_quantity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>04/22/2016 16:03:07</td>\n",
       "      <td>Latte</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>09/20/2016 09:24:23</td>\n",
       "      <td>Latte</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12/27/2016 13:09:00</td>\n",
       "      <td>Cappuccino, Unknown</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10/03/2016 10:17:46</td>\n",
       "      <td>New Orleans Iced Coffee (cup)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>09/17/2016 14:33:18</td>\n",
       "      <td>Sparkling Water</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>07/07/2016 08:06:00</td>\n",
       "      <td>Chemex Coffee</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>03/17/2016 08:15:40</td>\n",
       "      <td>Drip Coffee</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>09/09/2016 11:11:00</td>\n",
       "      <td>Cappuccino, Unknown</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>02/23/2016 11:05:25</td>\n",
       "      <td>New Orleans Iced Coffee (cup)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>03/27/2016 09:57:48</td>\n",
       "      <td>Apple Juice</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      local_created_at                      item_name  net_quantity\n",
       "0  04/22/2016 16:03:07                          Latte             1\n",
       "1  09/20/2016 09:24:23                          Latte             1\n",
       "2  12/27/2016 13:09:00            Cappuccino, Unknown             1\n",
       "3  10/03/2016 10:17:46  New Orleans Iced Coffee (cup)             2\n",
       "4  09/17/2016 14:33:18                Sparkling Water             1\n",
       "5  07/07/2016 08:06:00                  Chemex Coffee             1\n",
       "6  03/17/2016 08:15:40                    Drip Coffee             2\n",
       "7  09/09/2016 11:11:00            Cappuccino, Unknown             1\n",
       "8  02/23/2016 11:05:25  New Orleans Iced Coffee (cup)             1\n",
       "9  03/27/2016 09:57:48                    Apple Juice             1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "morse.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is the collection data of temperature in 2016, making it ready to use for further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then merge weather data with the original data, make it clean for well-formatted for analysis, such as the date-formatting of the create_time. The temperature has to the in one day, so transform the \"local_created_at\" should be necessary, or the two table cannot be merged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the comparably small dataset, EC2 is not necessary. I first used Pandas SQL for the questions, which can be copied and run in any computer. However, because of its terrible apparence, I turned to Spark. I used Spark SQL for the questions again, with different queries, because their languages are slightly different from each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per the two questions, I understand you are interested in the relationship of sales and weather. I used Spark Linear Regression to do prediction with item_name and temperature, and it shows the independent variables don't work well in the model. I think the assumption should have worked, because it is logical. The reason of the conclusion is the weather pattern in Oakland. In other words, it doesn't change dramastically all the time, so slight difference can be reasonably ingored by people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temperature Crawling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2016 Oakland Weather History\n",
    "https://www.usclimatedata.com/climate/oakland/california/united-states/usca2500/2016/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import html\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "days=[]\n",
    "for i in range(1,13):\n",
    "    web='https://www.usclimatedata.com/climate/oakland/california/united-states/usca2500/2016/'+str(i)\n",
    "    page = requests.get(web)\n",
    "    tree = html.fromstring(page.content)\n",
    "    day=tree.xpath('//td[@class=\"align_left daily_climate_table_td_day\"]/text()')\n",
    "    days.extend(day[31:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1 jan 2016',\n",
       " '2 jan 2016',\n",
       " '3 jan 2016',\n",
       " '4 jan 2016',\n",
       " '5 jan 2016',\n",
       " '6 jan 2016',\n",
       " '7 jan 2016',\n",
       " '8 jan 2016',\n",
       " '9 jan 2016',\n",
       " '10 jan 2016',\n",
       " '11 jan 2016',\n",
       " '12 jan 2016',\n",
       " '13 jan 2016',\n",
       " '14 jan 2016',\n",
       " '15 jan 2016',\n",
       " '16 jan 2016',\n",
       " '17 jan 2016',\n",
       " '18 jan 2016',\n",
       " '19 jan 2016',\n",
       " '20 jan 2016',\n",
       " '21 jan 2016',\n",
       " '22 jan 2016',\n",
       " '23 jan 2016',\n",
       " '24 jan 2016',\n",
       " '25 jan 2016',\n",
       " '26 jan 2016',\n",
       " '27 jan 2016',\n",
       " '28 jan 2016',\n",
       " '29 jan 2016',\n",
       " '30 jan 2016',\n",
       " '31 jan 2016',\n",
       " '1 feb 2016',\n",
       " '2 feb 2016',\n",
       " '3 feb 2016',\n",
       " '4 feb 2016',\n",
       " '5 feb 2016',\n",
       " '6 feb 2016',\n",
       " '7 feb 2016',\n",
       " '8 feb 2016',\n",
       " '9 feb 2016',\n",
       " '10 feb 2016',\n",
       " '11 feb 2016',\n",
       " '12 feb 2016',\n",
       " '13 feb 2016',\n",
       " '14 feb 2016',\n",
       " '15 feb 2016',\n",
       " '16 feb 2016',\n",
       " '17 feb 2016',\n",
       " '18 feb 2016',\n",
       " '19 feb 2016',\n",
       " '20 feb 2016',\n",
       " '21 feb 2016',\n",
       " '22 feb 2016',\n",
       " '23 feb 2016',\n",
       " '24 feb 2016',\n",
       " '25 feb 2016',\n",
       " '26 feb 2016',\n",
       " '27 feb 2016',\n",
       " '28 feb 2016',\n",
       " '1 mar 2016',\n",
       " '2 mar 2016',\n",
       " '3 mar 2016',\n",
       " '4 mar 2016',\n",
       " '5 mar 2016',\n",
       " '6 mar 2016',\n",
       " '7 mar 2016',\n",
       " '8 mar 2016',\n",
       " '9 mar 2016',\n",
       " '10 mar 2016',\n",
       " '11 mar 2016',\n",
       " '12 mar 2016',\n",
       " '13 mar 2016',\n",
       " '14 mar 2016',\n",
       " '15 mar 2016',\n",
       " '16 mar 2016',\n",
       " '17 mar 2016',\n",
       " '18 mar 2016',\n",
       " '19 mar 2016',\n",
       " '20 mar 2016',\n",
       " '21 mar 2016',\n",
       " '22 mar 2016',\n",
       " '23 mar 2016',\n",
       " '24 mar 2016',\n",
       " '25 mar 2016',\n",
       " '26 mar 2016',\n",
       " '27 mar 2016',\n",
       " '28 mar 2016',\n",
       " '29 mar 2016',\n",
       " '30 mar 2016',\n",
       " '31 mar 2016',\n",
       " '1 apr 2016',\n",
       " '2 apr 2016',\n",
       " '3 apr 2016',\n",
       " '4 apr 2016',\n",
       " '5 apr 2016',\n",
       " '6 apr 2016',\n",
       " '7 apr 2016',\n",
       " '8 apr 2016',\n",
       " '9 apr 2016',\n",
       " '10 apr 2016',\n",
       " '11 apr 2016',\n",
       " '12 apr 2016',\n",
       " '13 apr 2016',\n",
       " '14 apr 2016',\n",
       " '15 apr 2016',\n",
       " '16 apr 2016',\n",
       " '17 apr 2016',\n",
       " '18 apr 2016',\n",
       " '19 apr 2016',\n",
       " '20 apr 2016',\n",
       " '21 apr 2016',\n",
       " '22 apr 2016',\n",
       " '23 apr 2016',\n",
       " '24 apr 2016',\n",
       " '25 apr 2016',\n",
       " '26 apr 2016',\n",
       " '27 apr 2016',\n",
       " '28 apr 2016',\n",
       " '29 apr 2016',\n",
       " '30 apr 2016',\n",
       " '1 may 2016',\n",
       " '2 may 2016',\n",
       " '3 may 2016',\n",
       " '4 may 2016',\n",
       " '5 may 2016',\n",
       " '6 may 2016',\n",
       " '7 may 2016',\n",
       " '8 may 2016',\n",
       " '9 may 2016',\n",
       " '10 may 2016',\n",
       " '11 may 2016',\n",
       " '12 may 2016',\n",
       " '13 may 2016',\n",
       " '14 may 2016',\n",
       " '15 may 2016',\n",
       " '16 may 2016',\n",
       " '17 may 2016',\n",
       " '18 may 2016',\n",
       " '19 may 2016',\n",
       " '20 may 2016',\n",
       " '21 may 2016',\n",
       " '22 may 2016',\n",
       " '23 may 2016',\n",
       " '24 may 2016',\n",
       " '25 may 2016',\n",
       " '26 may 2016',\n",
       " '27 may 2016',\n",
       " '28 may 2016',\n",
       " '29 may 2016',\n",
       " '30 may 2016',\n",
       " '31 may 2016',\n",
       " '1 jun 2016',\n",
       " '2 jun 2016',\n",
       " '3 jun 2016',\n",
       " '4 jun 2016',\n",
       " '5 jun 2016',\n",
       " '6 jun 2016',\n",
       " '7 jun 2016',\n",
       " '8 jun 2016',\n",
       " '9 jun 2016',\n",
       " '10 jun 2016',\n",
       " '11 jun 2016',\n",
       " '12 jun 2016',\n",
       " '13 jun 2016',\n",
       " '14 jun 2016',\n",
       " '15 jun 2016',\n",
       " '16 jun 2016',\n",
       " '17 jun 2016',\n",
       " '18 jun 2016',\n",
       " '19 jun 2016',\n",
       " '20 jun 2016',\n",
       " '21 jun 2016',\n",
       " '22 jun 2016',\n",
       " '23 jun 2016',\n",
       " '24 jun 2016',\n",
       " '25 jun 2016',\n",
       " '26 jun 2016',\n",
       " '27 jun 2016',\n",
       " '28 jun 2016',\n",
       " '29 jun 2016',\n",
       " '30 jun 2016',\n",
       " '1 jul 2016',\n",
       " '2 jul 2016',\n",
       " '3 jul 2016',\n",
       " '4 jul 2016',\n",
       " '5 jul 2016',\n",
       " '6 jul 2016',\n",
       " '7 jul 2016',\n",
       " '8 jul 2016',\n",
       " '9 jul 2016',\n",
       " '10 jul 2016',\n",
       " '11 jul 2016',\n",
       " '12 jul 2016',\n",
       " '13 jul 2016',\n",
       " '14 jul 2016',\n",
       " '15 jul 2016',\n",
       " '16 jul 2016',\n",
       " '17 jul 2016',\n",
       " '18 jul 2016',\n",
       " '19 jul 2016',\n",
       " '20 jul 2016',\n",
       " '21 jul 2016',\n",
       " '22 jul 2016',\n",
       " '23 jul 2016',\n",
       " '24 jul 2016',\n",
       " '25 jul 2016',\n",
       " '26 jul 2016',\n",
       " '27 jul 2016',\n",
       " '28 jul 2016',\n",
       " '29 jul 2016',\n",
       " '30 jul 2016',\n",
       " '31 jul 2016',\n",
       " '1 aug 2016',\n",
       " '2 aug 2016',\n",
       " '3 aug 2016',\n",
       " '4 aug 2016',\n",
       " '5 aug 2016',\n",
       " '6 aug 2016',\n",
       " '7 aug 2016',\n",
       " '8 aug 2016',\n",
       " '9 aug 2016',\n",
       " '10 aug 2016',\n",
       " '11 aug 2016',\n",
       " '12 aug 2016',\n",
       " '13 aug 2016',\n",
       " '14 aug 2016',\n",
       " '15 aug 2016',\n",
       " '16 aug 2016',\n",
       " '17 aug 2016',\n",
       " '18 aug 2016',\n",
       " '19 aug 2016',\n",
       " '20 aug 2016',\n",
       " '21 aug 2016',\n",
       " '22 aug 2016',\n",
       " '23 aug 2016',\n",
       " '24 aug 2016',\n",
       " '25 aug 2016',\n",
       " '26 aug 2016',\n",
       " '27 aug 2016',\n",
       " '28 aug 2016',\n",
       " '29 aug 2016',\n",
       " '30 aug 2016',\n",
       " '31 aug 2016',\n",
       " '1 sep 2016',\n",
       " '2 sep 2016',\n",
       " '3 sep 2016',\n",
       " '4 sep 2016',\n",
       " '5 sep 2016',\n",
       " '6 sep 2016',\n",
       " '7 sep 2016',\n",
       " '8 sep 2016',\n",
       " '9 sep 2016',\n",
       " '10 sep 2016',\n",
       " '11 sep 2016',\n",
       " '12 sep 2016',\n",
       " '13 sep 2016',\n",
       " '14 sep 2016',\n",
       " '15 sep 2016',\n",
       " '16 sep 2016',\n",
       " '17 sep 2016',\n",
       " '18 sep 2016',\n",
       " '19 sep 2016',\n",
       " '20 sep 2016',\n",
       " '21 sep 2016',\n",
       " '22 sep 2016',\n",
       " '23 sep 2016',\n",
       " '24 sep 2016',\n",
       " '25 sep 2016',\n",
       " '26 sep 2016',\n",
       " '27 sep 2016',\n",
       " '28 sep 2016',\n",
       " '29 sep 2016',\n",
       " '30 sep 2016',\n",
       " '1 oct 2016',\n",
       " '2 oct 2016',\n",
       " '3 oct 2016',\n",
       " '4 oct 2016',\n",
       " '5 oct 2016',\n",
       " '6 oct 2016',\n",
       " '7 oct 2016',\n",
       " '8 oct 2016',\n",
       " '9 oct 2016',\n",
       " '10 oct 2016',\n",
       " '11 oct 2016',\n",
       " '12 oct 2016',\n",
       " '13 oct 2016',\n",
       " '14 oct 2016',\n",
       " '15 oct 2016',\n",
       " '16 oct 2016',\n",
       " '17 oct 2016',\n",
       " '18 oct 2016',\n",
       " '19 oct 2016',\n",
       " '20 oct 2016',\n",
       " '21 oct 2016',\n",
       " '22 oct 2016',\n",
       " '23 oct 2016',\n",
       " '24 oct 2016',\n",
       " '25 oct 2016',\n",
       " '26 oct 2016',\n",
       " '27 oct 2016',\n",
       " '28 oct 2016',\n",
       " '29 oct 2016',\n",
       " '30 oct 2016',\n",
       " '31 oct 2016',\n",
       " '1 nov 2016',\n",
       " '2 nov 2016',\n",
       " '3 nov 2016',\n",
       " '4 nov 2016',\n",
       " '5 nov 2016',\n",
       " '6 nov 2016',\n",
       " '7 nov 2016',\n",
       " '8 nov 2016',\n",
       " '9 nov 2016',\n",
       " '10 nov 2016',\n",
       " '11 nov 2016',\n",
       " '12 nov 2016',\n",
       " '13 nov 2016',\n",
       " '14 nov 2016',\n",
       " '15 nov 2016',\n",
       " '16 nov 2016',\n",
       " '17 nov 2016',\n",
       " '18 nov 2016',\n",
       " '19 nov 2016',\n",
       " '20 nov 2016',\n",
       " '21 nov 2016',\n",
       " '22 nov 2016',\n",
       " '23 nov 2016',\n",
       " '24 nov 2016',\n",
       " '25 nov 2016',\n",
       " '26 nov 2016',\n",
       " '27 nov 2016',\n",
       " '28 nov 2016',\n",
       " '29 nov 2016',\n",
       " '30 nov 2016',\n",
       " '1 dec 2016',\n",
       " '2 dec 2016',\n",
       " '3 dec 2016',\n",
       " '4 dec 2016',\n",
       " '5 dec 2016',\n",
       " '6 dec 2016',\n",
       " '7 dec 2016',\n",
       " '8 dec 2016',\n",
       " '9 dec 2016',\n",
       " '10 dec 2016',\n",
       " '11 dec 2016',\n",
       " '12 dec 2016',\n",
       " '13 dec 2016',\n",
       " '14 dec 2016',\n",
       " '15 dec 2016',\n",
       " '16 dec 2016',\n",
       " '17 dec 2016',\n",
       " '18 dec 2016',\n",
       " '19 dec 2016',\n",
       " '20 dec 2016',\n",
       " '21 dec 2016',\n",
       " '22 dec 2016',\n",
       " '23 dec 2016',\n",
       " '24 dec 2016',\n",
       " '25 dec 2016',\n",
       " '26 dec 2016',\n",
       " '27 dec 2016',\n",
       " '28 dec 2016',\n",
       " '29 dec 2016',\n",
       " '30 dec 2016',\n",
       " '31 dec 2016']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "format date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "temperature on 2016-02-29 was missing in the website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date, timedelta\n",
    "\n",
    "d1 = date(2016, 1, 1)  # start date\n",
    "d2 = date(2016, 12, 31)  # end date\n",
    "\n",
    "delta = d2 - d1         # timedelta\n",
    "\n",
    "dates=[]\n",
    "for i in range(delta.days + 1):\n",
    "    date=d1 + timedelta(i)\n",
    "    dates.append(date)\n",
    "    #print(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "366"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "highs=[]\n",
    "for i in range(1,13):\n",
    "    web='https://www.usclimatedata.com/climate/oakland/california/united-states/usca2500/2016/'+str(i)\n",
    "    page = requests.get(web)\n",
    "    tree = html.fromstring(page.content)\n",
    "    high =tree.xpath('//td[@class=\"align_right climate_table_data_td temperature_red \"]/text()')\n",
    "    highs.extend(high[31:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data backfill\n",
    "The temperatere of close days should be like each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace null values with the closest day's temperatere\n",
    "def replace(lst):\n",
    "    curr=lst[0]\n",
    "    for i in range(len(lst)):\n",
    "        if lst[i]=='-':\n",
    "            lst[i]=curr\n",
    "        else:\n",
    "            curr=lst[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace(highs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "highest=highs[:59]+[highs[59]]+highs[59:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_int(lst):\n",
    "    for i in range(len(lst)):\n",
    "        a=lst[i].split('.')\n",
    "        lst[i]=int(a[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_int(highest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "lows=[]\n",
    "for i in range(1,13):\n",
    "    web='https://www.usclimatedata.com/climate/oakland/california/united-states/usca2500/2016/'+str(i)\n",
    "    page = requests.get(web)\n",
    "    tree = html.fromstring(page.content)\n",
    "    low =tree.xpath('//td[@class=\"align_right climate_table_data_td temperature_blue\"]/text()')\n",
    "    lows.extend(low[31:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace(lows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowest=lows[:59]+[lows[59]]+lows[59:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_int(lowest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature=pd.DataFrame(dates,columns=['date'])\n",
    "temperature['High'] = pd.Series(highest, index=temperature.index)\n",
    "temperature['Low'] = pd.Series(lowest, index=temperature.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>55</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-01-02</td>\n",
       "      <td>55</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-01-03</td>\n",
       "      <td>62</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-01-04</td>\n",
       "      <td>62</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-01-05</td>\n",
       "      <td>57</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date  High  Low\n",
       "0  2016-01-01    55   39\n",
       "1  2016-01-02    55   44\n",
       "2  2016-01-03    62   46\n",
       "3  2016-01-04    62   46\n",
       "4  2016-01-05    57   48"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temperature.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge Temperature with Original Morse Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "morse['date'] = morse['local_created_at'].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "morse_tmp = pd.merge(morse, temperature, on=['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>local_created_at</th>\n",
       "      <th>item_name</th>\n",
       "      <th>net_quantity</th>\n",
       "      <th>date</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-04-22 16:03:07</td>\n",
       "      <td>Latte</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-04-22</td>\n",
       "      <td>63</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-04-22 10:54:16</td>\n",
       "      <td>Drip Coffee</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-04-22</td>\n",
       "      <td>63</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-04-22 14:05:23</td>\n",
       "      <td>Gibraltar</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-04-22</td>\n",
       "      <td>63</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-04-22 08:15:55</td>\n",
       "      <td>Latte</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-04-22</td>\n",
       "      <td>63</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-04-22 16:36:54</td>\n",
       "      <td>New Orleans Iced Coffee (cup)</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-04-22</td>\n",
       "      <td>63</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2016-04-22 07:57:24</td>\n",
       "      <td>Drip Coffee</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-04-22</td>\n",
       "      <td>63</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2016-04-22 08:23:28</td>\n",
       "      <td>Drip Coffee</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-04-22</td>\n",
       "      <td>63</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2016-04-22 07:58:55</td>\n",
       "      <td>Latte</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-04-22</td>\n",
       "      <td>63</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2016-04-22 15:32:21</td>\n",
       "      <td>Apple Juice</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-04-22</td>\n",
       "      <td>63</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2016-04-22 11:37:48</td>\n",
       "      <td>Drip Coffee</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-04-22</td>\n",
       "      <td>63</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     local_created_at                      item_name  net_quantity  \\\n",
       "0 2016-04-22 16:03:07                          Latte             1   \n",
       "1 2016-04-22 10:54:16                    Drip Coffee             1   \n",
       "2 2016-04-22 14:05:23                      Gibraltar             1   \n",
       "3 2016-04-22 08:15:55                          Latte             1   \n",
       "4 2016-04-22 16:36:54  New Orleans Iced Coffee (cup)             1   \n",
       "5 2016-04-22 07:57:24                    Drip Coffee             1   \n",
       "6 2016-04-22 08:23:28                    Drip Coffee             2   \n",
       "7 2016-04-22 07:58:55                          Latte             1   \n",
       "8 2016-04-22 15:32:21                    Apple Juice             1   \n",
       "9 2016-04-22 11:37:48                    Drip Coffee             1   \n",
       "\n",
       "         date  High  Low  \n",
       "0  2016-04-22    63   55  \n",
       "1  2016-04-22    63   55  \n",
       "2  2016-04-22    63   55  \n",
       "3  2016-04-22    63   55  \n",
       "4  2016-04-22    63   55  \n",
       "5  2016-04-22    63   55  \n",
       "6  2016-04-22    63   55  \n",
       "7  2016-04-22    63   55  \n",
       "8  2016-04-22    63   55  \n",
       "9  2016-04-22    63   55  "
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "morse_tmp.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "morse_tmp['Average'] = morse_tmp.apply(lambda row: int((row.High + row.Low)/2), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>local_created_at</th>\n",
       "      <th>item_name</th>\n",
       "      <th>net_quantity</th>\n",
       "      <th>date</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-04-22 16:03:07</td>\n",
       "      <td>Latte</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-04-22</td>\n",
       "      <td>63</td>\n",
       "      <td>55</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-04-22 10:54:16</td>\n",
       "      <td>Drip Coffee</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-04-22</td>\n",
       "      <td>63</td>\n",
       "      <td>55</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-04-22 14:05:23</td>\n",
       "      <td>Gibraltar</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-04-22</td>\n",
       "      <td>63</td>\n",
       "      <td>55</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-04-22 08:15:55</td>\n",
       "      <td>Latte</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-04-22</td>\n",
       "      <td>63</td>\n",
       "      <td>55</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-04-22 16:36:54</td>\n",
       "      <td>New Orleans Iced Coffee (cup)</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-04-22</td>\n",
       "      <td>63</td>\n",
       "      <td>55</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     local_created_at                      item_name  net_quantity  \\\n",
       "0 2016-04-22 16:03:07                          Latte             1   \n",
       "1 2016-04-22 10:54:16                    Drip Coffee             1   \n",
       "2 2016-04-22 14:05:23                      Gibraltar             1   \n",
       "3 2016-04-22 08:15:55                          Latte             1   \n",
       "4 2016-04-22 16:36:54  New Orleans Iced Coffee (cup)             1   \n",
       "\n",
       "         date  High  Low  Average  \n",
       "0  2016-04-22    63   55       59  \n",
       "1  2016-04-22    63   55       59  \n",
       "2  2016-04-22    63   55       59  \n",
       "3  2016-04-22    63   55       59  \n",
       "4  2016-04-22    63   55       59  "
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "morse_tmp.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "morse_tmp.to_csv('morse_tmp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "morse_new=pd.read_csv('morse_tmp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=morse_new.groupby('date').agg({'net_quantity':'sum', 'High':'mean', 'Low':'mean', 'Average':'mean'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('morse_sum.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Partioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will be more about schema design. However, the dataset is not that large that worth different tables for data partioning. But as far as I am concerned, I'd like to design, Item table(with item_names), Transaction table (create_time, item_name, new_quantity, date), Weather table (with date highest, lowest, average temperature)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main limitation is the values in net_quantity col, most of those are 1 or 2, makeing it inefficient to seperate to different tables, and it will cost more time to select and join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "morse_tmp=pd.read_csv('morse_tmp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "engine = create_engine('sqlite://', echo=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "morse_tmp.to_sql('morse_tmp', con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0, '2016-04-22 16:03:07', 'Latte', 1, '2016-04-22', 63, 55, 59),\n",
       " (1, 1, '2016-04-22 10:54:16', 'Drip Coffee', 1, '2016-04-22', 63, 55, 59),\n",
       " (2, 2, '2016-04-22 14:05:23', 'Gibraltar', 1, '2016-04-22', 63, 55, 59),\n",
       " (3, 3, '2016-04-22 08:15:55', 'Latte', 1, '2016-04-22', 63, 55, 59),\n",
       " (4, 4, '2016-04-22 16:36:54', 'New Orleans Iced Coffee (cup)', 1, '2016-04-22', 63, 55, 59)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.execute(\"SELECT * FROM morse_tmp LIMIT 5\").fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "engine = create_engine('sqlite://', echo=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "morse_tmp.to_sql('morse_tmp', con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, '2016-04-22 16:03:07.000000', 'Latte', 1, '2016-04-22', 63, 55, 59),\n",
       " (1, '2016-04-22 10:54:16.000000', 'Drip Coffee', 1, '2016-04-22', 63, 55, 59),\n",
       " (2, '2016-04-22 14:05:23.000000', 'Gibraltar', 1, '2016-04-22', 63, 55, 59),\n",
       " (3, '2016-04-22 08:15:55.000000', 'Latte', 1, '2016-04-22', 63, 55, 59),\n",
       " (4, '2016-04-22 16:36:54.000000', 'New Orleans Iced Coffee (cup)', 1, '2016-04-22', 63, 55, 59)]"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.execute(\"SELECT * FROM morse_tmp LIMIT 5\").fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First question\n",
    "Write a SQL query to generate the report of best-sellers in the following form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(60.49268018018018, 'Latte', 1945)]"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.execute(\"SELECT tmp as Temperature, item_name, MAX(total) as number_sold FROM (SELECT AVG(Average) as tmp, item_name, sum(net_quantity) as total FROM morse_tmp GROUP BY item_name) temp\").fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second question\n",
    "Write a SQL query to generate the average change on the number of items sold when the temperature is\n",
    "colder by 2 degrees Farenheit or warmer by 2 degrees Farenheit the next day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Americano', -0.10726351351351351, -0.10726351351351351),\n",
       " ('Apple Juice', -0.08775981524249422, -0.08775981524249422),\n",
       " ('Au Lait, Unknown', -0.1033210332103321, -0.1033210332103321),\n",
       " ('Cappuccino, Unknown', -0.013879408418657566, -0.013879408418657566),\n",
       " ('Cascara Fizz', -0.08882521489971347, -0.08882521489971347),\n",
       " ('Chemex Coffee', -0.025252525252525252, -0.025252525252525252),\n",
       " ('Drip Coffee', 0.02336754749945403, 0.02336754749945403),\n",
       " ('Espresso', -0.1291053227633069, -0.1291053227633069),\n",
       " ('Gibraltar', -0.11982881597717546, -0.11982881597717546),\n",
       " ('Hot Chocolate', -0.034482758620689655, -0.034482758620689655),\n",
       " ('Latte', -0.045156156753530935, -0.045156156753530935),\n",
       " ('Macchiato', 0.20679012345679013, 0.20679012345679013),\n",
       " ('Milk', -0.009615384615384616, -0.009615384615384616),\n",
       " ('Mocha', -0.05569620253164557, -0.05569620253164557),\n",
       " ('NOLA Carton', 0.5806451612903226, 0.5806451612903226),\n",
       " ('New Orleans Iced Coffee (cup)', 0.1372634326734827, 0.1372634326734827),\n",
       " ('S.O. Iced - 4 oz Oji', -0.09822866344605476, -0.09822866344605476),\n",
       " ('S.O. Iced Coffee', -0.05231984205330701, -0.05231984205330701),\n",
       " ('Shakerato', -0.045454545454545456, -0.045454545454545456),\n",
       " ('Sparkling Water', -0.10875331564986737, -0.10875331564986737),\n",
       " ('Still Water', -0.10869565217391304, -0.10869565217391304),\n",
       " ('Tea', -0.12535612535612536, -0.12535612535612536)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.execute(\"SELECt c.item_name, warm, cold from (SELECT a.item_name,AVG(a.net_quantity-b.net_quantity) as warm from morse_tmp a JOIN morse_tmp b ON JulianDay(a.date)-JulianDay(b.date)=1 WHERE a.Average-b.Average=-2 GROUP BY a.item_name) c join (SELECT m.item_name,AVG(m.net_quantity-n.net_quantity) as cold from morse_tmp m JOIN morse_tmp n ON JulianDay(m.date)-JulianDay(n.date)=1 WHERE m.Average-n.Average=-2 GROUP BY m.item_name) d on c.item_name=d.item_name\").fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.types import *\n",
    "sc =SparkContext()\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+------------+--------------------+----+---+-------+\n",
      "|_c0|    local_created_at|           item_name|net_quantity|                date|High|Low|Average|\n",
      "+---+--------------------+--------------------+------------+--------------------+----+---+-------+\n",
      "|  0|2016-04-22 16:03:...|               Latte|           1|2016-04-22 00:00:...|  63| 55|     59|\n",
      "|  1|2016-04-22 10:54:...|         Drip Coffee|           1|2016-04-22 00:00:...|  63| 55|     59|\n",
      "|  2|2016-04-22 14:05:...|           Gibraltar|           1|2016-04-22 00:00:...|  63| 55|     59|\n",
      "|  3|2016-04-22 08:15:...|               Latte|           1|2016-04-22 00:00:...|  63| 55|     59|\n",
      "|  4|2016-04-22 16:36:...|New Orleans Iced ...|           1|2016-04-22 00:00:...|  63| 55|     59|\n",
      "|  5|2016-04-22 07:57:...|         Drip Coffee|           1|2016-04-22 00:00:...|  63| 55|     59|\n",
      "|  6|2016-04-22 08:23:...|         Drip Coffee|           2|2016-04-22 00:00:...|  63| 55|     59|\n",
      "|  7|2016-04-22 07:58:...|               Latte|           1|2016-04-22 00:00:...|  63| 55|     59|\n",
      "|  8|2016-04-22 15:32:...|         Apple Juice|           1|2016-04-22 00:00:...|  63| 55|     59|\n",
      "|  9|2016-04-22 11:37:...|         Drip Coffee|           1|2016-04-22 00:00:...|  63| 55|     59|\n",
      "+---+--------------------+--------------------+------------+--------------------+----+---+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = sqlContext.read.load('morse_tmp.csv', format='com.databricks.spark.csv', header='true', inferSchema='true')\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- local_created_at: timestamp (nullable = true)\n",
      " |-- item_name: string (nullable = true)\n",
      " |-- net_quantity: integer (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- High: integer (nullable = true)\n",
      " |-- Low: integer (nullable = true)\n",
      " |-- Average: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"morse_tmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+------------+--------------------+----+---+-------+\n",
      "|_c0|    local_created_at|           item_name|net_quantity|                date|High|Low|Average|\n",
      "+---+--------------------+--------------------+------------+--------------------+----+---+-------+\n",
      "|  0|2016-04-22 16:03:...|               Latte|           1|2016-04-22 00:00:...|  63| 55|     59|\n",
      "|  1|2016-04-22 10:54:...|         Drip Coffee|           1|2016-04-22 00:00:...|  63| 55|     59|\n",
      "|  2|2016-04-22 14:05:...|           Gibraltar|           1|2016-04-22 00:00:...|  63| 55|     59|\n",
      "|  3|2016-04-22 08:15:...|               Latte|           1|2016-04-22 00:00:...|  63| 55|     59|\n",
      "|  4|2016-04-22 16:36:...|New Orleans Iced ...|           1|2016-04-22 00:00:...|  63| 55|     59|\n",
      "+---+--------------------+--------------------+------------+--------------------+----+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"SELECT * FROM morse_tmp LIMIT 5\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First question\n",
    "Write a SQL query to generate the report of best-sellers in the following form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------+-----------+\n",
      "|      Temperature|item_name|number_sold|\n",
      "+-----------------+---------+-----------+\n",
      "|60.49268018018018|    Latte|       1945|\n",
      "+-----------------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"SELECT tmp as Temperature, item_name, total as number_sold FROM (SELECT AVG(Average) as tmp, item_name, sum(net_quantity) as total FROM morse_tmp GROUP BY item_name) temp ORDER BY total DESC LIMIT 1\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second question\n",
    "Write a SQL query to generate the average change on the number of items sold when the temperature is\n",
    "colder by 2 degrees Farenheit or warmer by 2 degrees Farenheit the next day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|           item_name|                warm|                cold|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|         Apple Juice|-0.08775981524249422|-0.08775981524249422|\n",
      "|            Espresso| -0.1291053227633069| -0.1291053227633069|\n",
      "|                 Tea|-0.12535612535612536|-0.12535612535612536|\n",
      "|     Sparkling Water|-0.10875331564986737|-0.10875331564986737|\n",
      "|                Milk|-0.00961538461538...|-0.00961538461538...|\n",
      "|           Shakerato|-0.04545454545454...|-0.04545454545454...|\n",
      "|S.O. Iced - 4 oz Oji|-0.09822866344605476|-0.09822866344605476|\n",
      "|         Still Water|-0.10869565217391304|-0.10869565217391304|\n",
      "|               Mocha|-0.05569620253164557|-0.05569620253164557|\n",
      "|               Latte|-0.04515615675353...|-0.04515615675353...|\n",
      "|           Americano|-0.10726351351351351|-0.10726351351351351|\n",
      "|    Au Lait, Unknown| -0.1033210332103321| -0.1033210332103321|\n",
      "|           Gibraltar|-0.11982881597717546|-0.11982881597717546|\n",
      "|       Hot Chocolate|-0.03448275862068...|-0.03448275862068...|\n",
      "|    S.O. Iced Coffee|-0.05231984205330701|-0.05231984205330701|\n",
      "|         NOLA Carton|  0.5806451612903226|  0.5806451612903226|\n",
      "|        Cascara Fizz|-0.08882521489971347|-0.08882521489971347|\n",
      "| Cappuccino, Unknown|-0.01387940841865...|-0.01387940841865...|\n",
      "|           Macchiato| 0.20679012345679013| 0.20679012345679013|\n",
      "|New Orleans Iced ...|  0.1372634326734827|  0.1372634326734827|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"SELECt c.item_name, warm, cold from (SELECT a.item_name,AVG(a.net_quantity-b.net_quantity) as warm from morse_tmp a JOIN morse_tmp b ON DATEDIFF(a.date,b.date)=1 WHERE a.Average-b.Average=-2 GROUP BY a.item_name) c join (SELECT m.item_name,AVG(m.net_quantity-n.net_quantity) as cold from morse_tmp m JOIN morse_tmp n ON DATEDIFF(m.date,n.date)=1 WHERE m.Average-n.Average=-2 GROUP BY m.item_name) d on c.item_name=d.item_name\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis with ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows: 10000\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of rows: %d\" % df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seperate with different products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|count(DISTINCT item_name)|\n",
      "+-------------------------+\n",
      "|                       25|\n",
      "+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"SELECT count(DISTINCT item_name) FROM morse_tmp\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, RegexTokenizer\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+\n",
      "|            features|net_quantity|\n",
      "+--------------------+------------+\n",
      "|(28,[19,25,26,27]...|           1|\n",
      "|(28,[1,2,25,26,27...|           1|\n",
      "|(28,[23,25,26,27]...|           1|\n",
      "|(28,[19,25,26,27]...|           1|\n",
      "|(28,[0,1,11,19,25...|           1|\n",
      "+--------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "regexTokenizer_name = RegexTokenizer(inputCol=\"item_name\", outputCol=\"name\", pattern=\"\\\\W\")\n",
    "hashingTF_name = HashingTF(inputCol=\"name\", outputCol=\"rawName\", numFeatures=25)\n",
    "assembler = VectorAssembler(inputCols=['rawName', 'High', 'Low', 'Average'],outputCol='features')\n",
    "pipeline = Pipeline(stages=[regexTokenizer_name,hashingTF_name,assembler])\n",
    "pipelineFit = pipeline.fit(df)\n",
    "Morse = pipelineFit.transform(df)\n",
    "Morse=Morse.select(['features', 'net_quantity'])\n",
    "Morse.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Count: 6047\n",
      "Test Dataset Count: 3953\n"
     ]
    }
   ],
   "source": [
    "(training_data, test_data) = Morse.randomSplit([0.7, 0.3], seed = 100)\n",
    "print(\"Training Dataset Count: \" + str(trainingData.count()))\n",
    "print(\"Test Dataset Count: \" + str(testData.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]\n",
      "Intercept: 1.108476501490842\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "lr = LinearRegression(featuresCol = 'features', labelCol='net_quantity', maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "lr_model = lr.fit(training_data)\n",
    "print(\"Coefficients: \" + str(lr_model.coefficients))\n",
    "print(\"Intercept: \" + str(lr_model.intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.380768\n",
      "r2: -0.000000\n"
     ]
    }
   ],
   "source": [
    "trainingSummary = lr_model.summary\n",
    "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
    "print(\"r2: %f\" % trainingSummary.r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is reliable to say, it cannot be a useful prediction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict with Single day features, getting rid of item_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----+----+-------+--------------------+\n",
      "|net_quantity|High| Low|Average|                Date|\n",
      "+------------+----+----+-------+--------------------+\n",
      "|          29|80.0|57.0|   68.0|2016-06-20 00:00:...|\n",
      "|          30|61.0|51.0|   56.0|2016-12-10 00:00:...|\n",
      "|          55|62.0|46.0|   54.0|2016-11-23 00:00:...|\n",
      "|          36|70.0|55.0|   62.0|2016-08-16 00:00:...|\n",
      "|          41|72.0|57.0|   64.0|2016-11-11 00:00:...|\n",
      "|          30|75.0|59.0|   67.0|2016-09-02 00:00:...|\n",
      "|          23|66.0|54.0|   60.0|2016-04-11 00:00:...|\n",
      "|          26|66.0|53.0|   59.0|2016-05-12 00:00:...|\n",
      "|          26|80.0|50.0|   65.0|2016-03-17 00:00:...|\n",
      "|          27|70.0|55.0|   62.0|2016-05-29 00:00:...|\n",
      "+------------+----+----+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"SELECT SUM(net_quantity) as net_quantity, AVG(High) as High, AVG(Low) as Low, AVG(Average) as Average, Date FROM morse_tmp Group by date limit 10\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Above is what it will be for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+----+---+-------+\n",
      "|                date|net_quantity|High|Low|Average|\n",
      "+--------------------+------------+----+---+-------+\n",
      "|2016-01-23 00:00:...|           1|  61| 52|     56|\n",
      "|2016-01-24 00:00:...|           4|  62| 46|     54|\n",
      "|2016-02-04 00:00:...|           1|  63| 46|     54|\n",
      "|2016-02-05 00:00:...|           1|  70| 43|     56|\n",
      "|2016-02-06 00:00:...|          25|  70| 46|     58|\n",
      "|2016-02-07 00:00:...|          46|  73| 46|     59|\n",
      "|2016-02-08 00:00:...|          52|  81| 55|     68|\n",
      "|2016-02-09 00:00:...|          44|  73| 48|     60|\n",
      "|2016-02-10 00:00:...|          35|  71| 51|     61|\n",
      "|2016-02-11 00:00:...|          17|  73| 51|     62|\n",
      "+--------------------+------------+----+---+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "morse_sum = sqlContext.read.load('morse_sum.csv', format='com.databricks.spark.csv', header='true', inferSchema='true')\n",
    "morse_sum.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------+\n",
      "|        features|net_quantity|\n",
      "+----------------+------------+\n",
      "|[61.0,52.0,56.0]|           1|\n",
      "|[62.0,46.0,54.0]|           4|\n",
      "|[63.0,46.0,54.0]|           1|\n",
      "|[70.0,43.0,56.0]|           1|\n",
      "|[70.0,46.0,58.0]|          25|\n",
      "|[73.0,46.0,59.0]|          46|\n",
      "|[81.0,55.0,68.0]|          52|\n",
      "|[73.0,48.0,60.0]|          44|\n",
      "|[71.0,51.0,61.0]|          35|\n",
      "|[73.0,51.0,62.0]|          17|\n",
      "+----------------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "assembler = VectorAssembler(inputCols=['High', 'Low', 'Average'],outputCol='features')\n",
    "pipeline = Pipeline(stages=[assembler])\n",
    "pipelineFit = pipeline.fit(morse_sum)\n",
    "clean_Morse = pipelineFit.transform(morse_sum)\n",
    "clean_Morse=clean_Morse.select(['features', 'net_quantity'])\n",
    "clean_Morse.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Count: 232\n",
      "Test Dataset Count: 99\n"
     ]
    }
   ],
   "source": [
    "(training, test) = clean_Morse.randomSplit([0.7, 0.3], seed = 100)\n",
    "print(\"Training Dataset Count: \" + str(training.count()))\n",
    "print(\"Test Dataset Count: \" + str(test.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.0,-0.041752701873271444,0.0]\n",
      "Intercept: 35.48766655679977\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "lr = LinearRegression(featuresCol = 'features', labelCol='net_quantity', maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "lr_model = lr.fit(training)\n",
    "print(\"Coefficients: \" + str(lr_model.coefficients))\n",
    "print(\"Intercept: \" + str(lr_model.intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------+----------------+\n",
      "|       prediction|net_quantity|        features|\n",
      "+-----------------+------------+----------------+\n",
      "|33.94281658748873|          24|[55.0,37.0,46.0]|\n",
      "|33.65054767437583|          38|[55.0,44.0,49.0]|\n",
      "|33.56704227062929|          20|[55.0,46.0,50.0]|\n",
      "| 33.4000314631362|          32|[55.0,50.0,52.0]|\n",
      "| 33.4000314631362|          30|[57.0,50.0,53.0]|\n",
      "+-----------------+------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "R Squared (R2) on test data = 0.000669191\n"
     ]
    }
   ],
   "source": [
    "lr_predictions = lr_model.transform(test)\n",
    "lr_predictions.select(\"prediction\",\"net_quantity\",\"features\").show(5)\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "lr_evaluator = RegressionEvaluator(predictionCol=\"prediction\", \\\n",
    "                 labelCol=\"net_quantity\",metricName=\"r2\")\n",
    "print(\"R Squared (R2) on test data = %g\" % lr_evaluator.evaluate(lr_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 11.358334\n",
      "r2: 0.001003\n"
     ]
    }
   ],
   "source": [
    "trainingSummary = lr_model.summary\n",
    "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
    "print(\"r2: %f\" % trainingSummary.r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 10.8016\n"
     ]
    }
   ],
   "source": [
    "test_result = lr_model.evaluate(test)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % test_result.rootMeanSquaredError)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Both predictions looks far from decent conclusion\n",
    "It probably indicates that the temperature should not be a good feature in the model. By all means, the weather doesn't change dramastically in Oakland CA. So the assumption might work in the east coast."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
